---
title: "Question 3"
author: "David Fraire"
date: "3/12/2019"
output: html_document
---
# Data Import and Visualizations
```{r, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(FNN)
library(tidyverse)
library(datasets)
library(dbplyr)
library(dplyr)
library(ggformula)
library(ggplot2)
library(ggstance)
library(graphics)
library(grDevices)
library(lattice)
library(markdown)
library(Matrix)
library(methods)
library(mosaic)
library(mosaicData)
library(pander)
library(RColorBrewer)
library(rmarkdown)
library(stringr)
library(tidyr)
library(tidyverse)
library(utils)

onlinenews <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
onlinenews$viral <- as.numeric(onlinenews$shares > 1400)
colnames(onlinenews)

ON <- (onlinenews)

ON$viral <- ifelse(ON$shares > 1400, 1, 0)
View(ON)

head(ON$viral)

## Day of the Week split
ON$Monday <- ifelse(ON$weekday_is_monday== 1,1,0)
ON$Tuesday <- ifelse(ON$weekday_is_tuesday == 1, 2, 0)
ON$Wednesday <- ifelse(ON$weekday_is_wednesday == 1, 3, 0)
ON$Thursday <- ifelse(ON$weekday_is_thursday == 1, 4, 0)
ON$Friday <- ifelse(ON$weekday_is_friday == 1, 5, 0)
ON$Saturday <- ifelse(ON$weekday_is_saturday == 1, 6, 0)
ON$Sunday <- ifelse(ON$weekday_is_sunday == 1, 7, 0)
ON$weekday_num = ON$Monday + ON$Tuesday + ON$Wednesday + ON$Thursday + ON$Friday + ON$Saturday + ON$Sunday
ON$weekday = factor(ON$weekday_num, levels=1:7, 
                    labels=c("Monday","Tuesday","Wednesday", "Thursday","Friday", "Saturday", "Sunday"))

ON$Entertainment <- ifelse(ON$data_channel_is_entertainment== 1,1,0)
ON$Social <- ifelse(ON$data_channel_is_socmed == 1, 2, 0)
ON$World <- ifelse(ON$data_channel_is_world == 1, 3, 0)
ON$Lifestyle <- ifelse(ON$data_channel_is_lifestyle == 1, 4, 0)
ON$Business <- ifelse(ON$data_channel_is_bus == 1, 5, 0)
ON$Tech <- ifelse(ON$data_channel_is_tech == 1, 6, 0)
ON$Category_num = ON$Entertainment + ON$Social + ON$World + ON$Lifestyle + ON$Business + ON$Tech
ON$Category = factor(ON$Category_num, levels=0:6, 
                    labels=c("Misc","Entertainment","Social","World", "Lifestyle","Business", "Tech" ))
  
View(ON)

## PLOTS
ggplot(ON, aes(x=weekday))+
  geom_bar(aes(fill=factor(Category)))+
  labs(title="# of Articles published per day",
       x="Weekday",
       y="Number of Articles")+
  facet_wrap(~viral,nrow=2)


ggplot(ON, aes(x=Category, y=average_token_length))+
  geom_boxplot(aes(fill=weekday))


ggplot(ON, aes(x=Category, y=shares))+
  geom_boxplot(aes(fill=weekday))+
  ylim(0, 2000) ## Number of shares seems to be higher on Saturday/Sunday


# This graph shows us that for each category, articles published on Saturday have the highest shares. Maybe
## CATEGORY PLOTS
ggplot(ON, aes(x=Category, y=n_tokens_content))+
  geom_boxplot(aes(fill=factor(weekday)))+
  ylim(0,1000)#### Longer titles get more views on Sat/Sunday (same conclusion as the graph above)


ggplot(ON, aes(x=weekday, y=n_tokens_title))+
  geom_boxplot(aes(fill=factor(Category))) ## Over the course of the week, the number of letters in a title per category are not very different. This is inconclusive. 

ggplot(ON, aes(x=Category, y=average_token_length))+
  geom_boxplot(aes(fill=Category))+
  facet_wrap(~viral,nrow=2)  ### Not enough variance


ggplot(ON, aes(x=Category))+
  geom_bar(aes(fill=factor(Category)))+ 
  labs(title="Count of articles per category",
       y="Number of Shares") ## Number of articles per category
## Violin PLot of Shares per type of Article Category
ggplot(ON, aes(x=Category, y=num_imgs))+
  geom_violin(aes(fill=Category))+
  labs(title="Avg Negative Polarity per Article Category")+
  ylim(0,10)
  
## Bar plot of each category and 
ggplot(ON, aes(n_tokens_title))+
  geom_bar(aes(fill=factor(Category)), alpha=0.9) +
  xlim(0,20)
##Density Plot of Category
ggplot(ON, aes(shares))+
  geom_density(aes(fill=factor(Category)), alpha=0.5) +
  xlim(0,16000)



```

# Testing performance for all linear models w/ shares as outcome variable (lm1, lm2, and lm3)
```{r}
# Train/Test Split

n = nrow(onlinenews)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = onlinenews[train_cases,]
news_test = onlinenews[test_cases,]

## Fitting linear models on train
lm1 = lm(shares ~ average_token_length + num_imgs + num_videos + num_hrefs + weekday_is_saturday, data=news_train)
lm2 = lm(shares ~ num_hrefs*num_imgs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
lm3 = lm(shares ~ n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend, data=news_train)

summary(lm1)
summary(lm2)
summary(lm3)
#in sample
phat_train1 = predict(lm1, news_train) 
phat_train2 = predict(lm2, news_train) 
phat_train3 = predict(lm3, news_train) 

yhat_train1 = ifelse(phat_train1 >= 1400, 1,0)
yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)
yhat_train3 = ifelse(phat_train3 >= 1400, 1,0)


#in sample performance
confusion_in1 = table(y = news_train$viral, yhat = yhat_train1)
confusion_in2 = table(y = news_train$viral, yhat = yhat_train2)
confusion_in3 = table(y = news_train$viral, yhat = yhat_train3)

confusion_in1
confusion_in2
confusion_in3

sum(diag(confusion_in1))/sum(confusion_in1)
sum(diag(confusion_in2))/sum(confusion_in2)
sum(diag(confusion_in3))/sum(confusion_in3)

#out of sample
phat_test1 <- predict(lm1, news_test)
phat_test2 <- predict(lm2, news_test)
phat_test3 <- predict(lm3, news_test)

yhat_test1 = ifelse(phat_test1 > 1400, 1,0)
yhat_test2 = ifelse(phat_test2 > 1400, 1,0)
yhat_test3 = ifelse(phat_test3 > 1400, 1,0)

#out of sample performance
confusion_out1 = table(y = news_test$viral, yhat = yhat_test1)
confusion_out2 = table(y = news_test$viral, yhat = yhat_test2)
confusion_out3 = table(y = news_test$viral, yhat = yhat_test3)

confusion_out1
confusion_out2
confusion_out3

sum(diag(confusion_out1))/sum(confusion_out1)
sum(diag(confusion_out2))/sum(confusion_out2)
sum(diag(confusion_out3))/sum(confusion_out3)

```
# Testing Performance for best Linear Model (lm2)
```{r}

  
# Train/Test Splits
n = nrow(ON)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = ON[train_cases,]
news_test = ON[test_cases,]

## Fitting model to training data
lm2 = lm(shares ~ num_hrefs*num_imgs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)


#in sample

phat_train2 = predict(lm2, news_train) 
yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)

#in sample performance
confusion_in2 = table(y = news_train$viral, yhat = yhat_train2)

confusion_in2

sum(diag(confusion_in2))/sum(confusion_in2)

#out of sample
phat_test2 <- predict(lm2, news_test)

yhat_test2 = ifelse(phat_test2 >= 1400,1,0)

#out of sample performance
confusion_out2 = table(y = news_test$viral, yhat = yhat_test2)

confusion_out2

sum(diag(confusion_out2))/sum(confusion_out2)


```

#Generlized Linear Model 
```{r}

lm1 = glm(viral ~ average_token_length + num_imgs  + num_videos + num_hrefs + weekday_is_saturday , data=onlinenews, family=binomial)
lm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=onlinenews, family=binomial)
lm3 = glm(viral ~ (n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend), data=onlinenews, family=binomial)

summary(lm1)
summary(lm2)
summary(lm3)
yhat1 <- predict(lm1,onlinenews)
yhat2 <- predict(lm2,onlinenews)
yhat3 <- predict(lm3,onlinenews)



rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}


## TRAIN/TEST + Do Loop for each model on the training data
rmsevals <- do(100)*{
  n = nrow(onlinenews)
  n_train = round(0.7*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = onlinenews[train_cases,]
  news_test = onlinenews[test_cases,]
	
# Fit to the training data
  lm1train = glm(viral ~ average_token_length + num_imgs  + num_videos + num_hrefs + weekday_is_saturday , data=news_train, family=binomial)
  
  lm2train = glm(viral ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family=binomial)
  
  lm3train = glm(viral ~ n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend, data=news_train, family=binomial)

# Predictions out of sample
  yhat_test1 = predict(lm1train, news_test)
  yhat_test2 = predict(lm2train, news_test)
  yhat_test3 = predict(lm3train, news_test)

# Root mean-squared prediction error
  c(rmse(news_test$viral, yhat_testg1), rmse(news_test$viral, yhat_testg2),rmse(news_test$viral, yhat_testg3))
}


colMeans(rmsevals)

```


# Question 3 Part II: Classification Model using GLM Binomial 
```{r}
onlinenews <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
onlinenews$viral <- as.numeric(onlinenews$shares >= 1400)

ON <- na.omit(onlinenews)

ON$viral <- as.numeric(ON$shares >= 1400)

## Train/ Test split for Generalized model 
n = nrow(onlinenews)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = onlinenews[train_cases,]
news_test = onlinenews[test_cases,]

glm1 = glm(viral ~ average_token_length + num_imgs  + num_videos + num_hrefs + weekday_is_saturday, data=news_train, family='binomial')

glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')

glm3 = glm(viral ~ (n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend), data=news_train, family='binomial')



 
stargazer::stargazer(glm1, glm2, glm3, type= "text")


## In-Sample Performance

phat_gtrain1 = predict(glm1, news_train) 
phat_gtrain2 = predict(glm2, news_train) 
phat_gtrain3 = predict(glm3, news_train) 

yhat_gtrain1 = ifelse(phat_gtrain1>0.5, 1,0)
yhat_gtrain2 = ifelse(phat_gtrain2>0.5, 1,0)
yhat_gtrain3 = ifelse(phat_gtrain3>0.5, 1,0)

confusion_ing1 = table(y = news_train$viral, yhat = yhat_gtrain1)

confusion_ing2 = table(y = news_train$viral, yhat = yhat_gtrain2)

confusion_ing3 = table(y = news_train$viral, yhat = yhat_gtrain3)

confusion_ing1
confusion_ing2
confusion_ing3

sum(diag(confusion_ing1))/sum(confusion_ing1)
sum(diag(confusion_ing2))/sum(confusion_ing2)
sum(diag(confusion_ing3))/sum(confusion_ing3)

## Out of sample

phat_testg1 = predict(glm1, news_test)
phat_testg2 = predict(glm2, news_test)
phat_testg3 = predict(glm2, news_test)

yhat_testg1 = ifelse(phat_testg1 > 0.5,1,0)
yhat_testg2 = ifelse(phat_testg2 > 0.5,1,0)
yhat_testg3 = ifelse(phat_testg3 > 0.5,1,0)

confusion_outg1 = table(y = news_test$viral, yhat = yhat_testg1)
confusion_outg2 = table(y = news_test$viral, yhat = yhat_testg2)
confusion_outg3 = table(y = news_test$viral, yhat = yhat_testg3)

confusion_outg1
confusion_outg2
confusion_outg3

sum(diag(confusion_outg1))/sum(confusion_outg1)
sum(diag(confusion_outg2))/sum(confusion_outg2)
sum(diag(confusion_outg3))/sum(confusion_outg3)



```