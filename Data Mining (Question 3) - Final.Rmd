---
title: "Question 3"
author: "David Fraire"
date: "3/12/2019"
output: html_document
---
# Mashable Audit
The dataset used in the analysis describes ~39,640 observations of articles published on Mashable.com's website. In the original dataset there are 37 predictor variables that describe each article. 

To begin building our linear model, we decided to plot the data points in various visualizations to find any striking relationships across the data. We transformed the initial dataset by generating dummy variables indicating the specific category of article into a single categorical variable column. We repeated this same transformation to indicate the day of the week each article was published.  

After performing a simple count statistics, we learned that our dataset contained mostly articles that were published on Wednesday and that were classified under the general category of World, followed by Technology. Given this information, we decided to create a boxplot visualization to explore the relationship between the number of shares across the days of the week for each particular type of category. Across each category, by week, the highest number of shares was on the weekend (Saturday and Sunday). We then explored the number of raw shares across each category and found that articles in the dataset that were categorized as Social Media tended to have higher numbers of shares. The next categories that tended to demonstrate higher number of shares were Lifestyle and Tech articles. 


```{r, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(FNN)
library(tidyverse)
library(datasets)
library(dbplyr)
library(dplyr)
library(ggformula)
library(ggplot2)
library(ggstance)
library(graphics)
library(grDevices)
library(lattice)
library(markdown)
library(Matrix)
library(methods)
library(mosaic)
library(mosaicData)
library(pander)
library(RColorBrewer)
library(rmarkdown)
library(stringr)
library(tidyr)
library(tidyverse)
library(utils)

onlinenews <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
onlinenews$viral <- as.numeric(onlinenews$shares > 1400)
colnames(onlinenews)

ON <- (onlinenews)

ON$viral <- ifelse(ON$shares > 1400, 1, 0)
View(ON)

head(ON$viral)

## Day of the Week split
ON$Monday <- ifelse(ON$weekday_is_monday== 1,1,0)
ON$Tuesday <- ifelse(ON$weekday_is_tuesday == 1, 2, 0)
ON$Wednesday <- ifelse(ON$weekday_is_wednesday == 1, 3, 0)
ON$Thursday <- ifelse(ON$weekday_is_thursday == 1, 4, 0)
ON$Friday <- ifelse(ON$weekday_is_friday == 1, 5, 0)
ON$Saturday <- ifelse(ON$weekday_is_saturday == 1, 6, 0)
ON$Sunday <- ifelse(ON$weekday_is_sunday == 1, 7, 0)
ON$weekday_num = ON$Monday + ON$Tuesday + ON$Wednesday + ON$Thursday + ON$Friday + ON$Saturday + ON$Sunday
ON$weekday = factor(ON$weekday_num, levels=1:7, 
                    labels=c("Monday","Tuesday","Wednesday", "Thursday","Friday", "Saturday", "Sunday"))

ON$Entertainment <- ifelse(ON$data_channel_is_entertainment== 1,1,0)
ON$Social <- ifelse(ON$data_channel_is_socmed == 1, 2, 0)
ON$World <- ifelse(ON$data_channel_is_world == 1, 3, 0)
ON$Lifestyle <- ifelse(ON$data_channel_is_lifestyle == 1, 4, 0)
ON$Business <- ifelse(ON$data_channel_is_bus == 1, 5, 0)
ON$Tech <- ifelse(ON$data_channel_is_tech == 1, 6, 0)
ON$Category_num = ON$Entertainment + ON$Social + ON$World + ON$Lifestyle + ON$Business + ON$Tech
ON$Category = factor(ON$Category_num, levels=0:6, 
                    labels=c("Misc","Entertainment","Social","World", "Lifestyle","Business", "Tech" ))
  
View(ON)

## PLOTS
ggplot(ON, aes(x=weekday, y=shares))+
  geom_boxplot(aes(fill=Category))+
  ylim(0,2000)
  # Number of shares seems to be higher on Saturday/Sunday, gives us indication to add weekday

ggplot(ON, aes(weekday))+
  geom_bar(aes(fill=Category)) ## Count of articles per day

ggplot(ON, aes(Category))+
  geom_bar(aes(fill=Category))+
  labs(title="# of Articles published per Category",
       x="",
       y="Number of Articles")

## Violin Plot of Shares per type of Article Category
ggplot(ON, aes(x=Category, y=shares))+
  geom_violin(aes(fill=Category))+
  labs(title="Distribution of shares per Article Category",
       y="Shares",
       x="")+
  ylim(0,6000) ## This tells us that social media tends to have the higher number of shares out of all the categories
  


```


Given the visualizations, we hand-built a simple linear model including a mix of variables that were used to predict number of shares. The variables we chose to include were the following: number of hyperlinks, whether or not the article was published on the weekend, global rate of negative words, whether the article was Business, self reference average shares, whether the article was entertainment, number of keywords, and average negative polarity. Our visualizations showed a trend of higher shares on the weekend and seems to be very significant in our model. The particular type of article is a very deterministic part of prediciting if an article will go viral as certain types of catergories have higher shares. 


Next, we tested the in-and-out-of-sample performance with a predictive success rate of ~49.5%. We then changed the outcome variable to include the threshhold variable "Viral" to represent any articles that exceeded 1400 shares, the cut off for Mashable's viral measurement.

# Testing performance for all linear models w/ shares as outcome variable (lm1, lm2, and lm3)
```{r}
# Train/Test Split

n = nrow(onlinenews)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = onlinenews[train_cases,]
news_test = onlinenews[test_cases,]

## Fitting linear models on train
#lm1 = lm(shares ~ average_token_length + num_imgs + num_videos + num_hrefs + weekday_is_saturday, data=news_train)
lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
#lm3 = lm(shares ~ n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend, data=news_train)

#summary(lm1)
summary(lm2)
#summary(lm3)
#in sample
#phat_train1 = predict(lm1, news_train) 
phat_train2 = predict(lm2, news_train) 
#phat_train3 = predict(lm3, news_train) 

#yhat_train1 = ifelse(phat_train1 >= 1400, 1,0)
yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)
#yhat_train3 = ifelse(phat_train3 >= 1400, 1,0)


#in sample performance
#confusion_in1 = table(y = news_train$viral, yhat = yhat_train1)
confusion_in2 = table(y = news_train$viral, yhat = yhat_train2)
#confusion_in3 = table(y = news_train$viral, yhat = yhat_train3)

#confusion_in1
confusion_in2
#confusion_in3

#sum(diag(confusion_in1))/sum(confusion_in1)
sum(diag(confusion_in2))/sum(confusion_in2)
#sum(diag(confusion_in3))/sum(confusion_in3)

#out of sample
#phat_test1 <- predict(lm1, news_test)
phat_test2 <- predict(lm2, news_test)
#phat_test3 <- predict(lm3, news_test)

#yhat_test1 = ifelse(phat_test1 > 1400, 1,0)
yhat_test2 = ifelse(phat_test2 > 1400, 1,0)
#yhat_test3 = ifelse(phat_test3 > 1400, 1,0)

#out of sample performance
#confusion_out1 = table(y = news_test$viral, yhat = yhat_test1)
confusion_out2 = table(y = news_test$viral, yhat = yhat_test2)
#confusion_out3 = table(y = news_test$viral, yhat = yhat_test3)

#confusion_out1
confusion_out2
#confusion_out3

#sum(diag(confusion_out1))/sum(confusion_out1)
sum(diag(confusion_out2))/sum(confusion_out2)
#sum(diag(confusion_out3))/sum(confusion_out3)

```
# Testing Performance for best Linear Model (lm2)
```{r}
set.seed(123456)
library(mosaic)

IN.lm<- do(100)*{  
  # Train/Test Splits
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  ## Fitting model to training data
  lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
  #in sample
  phat_train2 = predict(lm2, news_train) 
  yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)
#in sample performance
  confusion_in2 = table(y = news_train$viral, yhat = yhat_train2)
  sum(diag(confusion_in2))/sum(confusion_in2)
}
confusion_in2
colMeans(IN.lm)


OUT.lm <- do(100)*{
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  ## Fitting model to training data
  lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
  phat_test2 <- predict(lm2, news_test)
  yhat_test2 = ifelse(phat_test2 >= 1400,1,0)
#out of sample performance
  confusion_out2 = table(y = news_test$viral, yhat = yhat_test2)
  sum(diag(confusion_out2))/sum(confusion_out2)
}

confusion_out2
colMeans(OUT.lm)
```






#Generlized Linear Model 
```{r}

glm1 = glm(viral ~ average_token_length + num_imgs  + num_videos + num_hrefs + weekday_is_saturday , data=ON, family=binomial)
glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=ON, family=binomial)
glm3 = glm(viral ~ (n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend), data=ON, family=binomial)

stargazer::stargazer(glm1, glm2, glm3, type="text")

56.92 - 49.44
56.82 - 49.46
```

In order to model for the binomial outcome variable "Viral", we switched to a Generalized Linear Model and tested three seperate regressions. The best performing model we created was "glm2". We proceeded to assess the performance of this model by average the results of 100 train/test splits. The results of this new model had a lift of +7.48% from our initial regression for the in-sample results and of +7.36% for the out-of-sample results. The second approach performs much better. (I am not sure why, ask Kylie???)


# Question 3 Part II: Classification Model using GLM Binomial 
```{r}

IN.glm <- do(100)*{
## Train/ Test split for Generalized model 
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')
## In-Sample Performance 
  phat_gtrain2 = predict(glm2, news_train) 
  yhat_gtrain2 = ifelse(phat_gtrain2>0.5, 1,0)
  confusion_ing2 = table(y = news_train$viral, yhat = yhat_gtrain2)
  sum(diag(confusion_ing2))/sum(confusion_ing2)
}

confusion_ing2
colMeans(IN.glm)

## Out of sample
OUT.glm <- do(100)*{
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')
  phat_testg2 = predict(glm2, news_test)
  yhat_testg2 = ifelse(phat_testg2 > 0.5,1,0)
  confusion_outg2 = table(y = news_test$viral, yhat = yhat_testg2)
  sum(diag(confusion_outg2))/sum(confusion_outg2)
}

confusion_outg2
colMeans(OUT.glm)
```